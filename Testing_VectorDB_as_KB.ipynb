{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Load raw PDF(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of PDF pages:  59\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH=\"./data/\"\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob='*.pdf',\n",
    "                             loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents=loader.load()\n",
    "    return documents\n",
    "\n",
    "documents=load_pdf_files(data=DATA_PATH)\n",
    "print(\"Length of PDF pages: \", len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks:  1599\n"
     ]
    }
   ],
   "source": [
    "def create_chunks(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,\n",
    "                                                 chunk_overlap=25)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks=create_chunks(extracted_data=documents)\n",
    "print(\"Length of Text Chunks: \", len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Create Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshitchaudhari/Downloads/my_Playground_/sample/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_model():\n",
    "    embedding_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embedding_model\n",
    "\n",
    "embedding_model=get_embedding_model()\n",
    "\n",
    "# Step 4: Store embeddings in FAISS\n",
    "DB_FAISS_PATH=\"vectorstore/db_faiss\"\n",
    "db=FAISS.from_documents(text_chunks, embedding_model)\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "def load_llm(huggingface_repo_id):\n",
    "    if not HF_TOKEN:\n",
    "        raise ValueError(\"HF_TOKEN environment variable is not set!\")\n",
    "    \n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=huggingface_repo_id,\n",
    "        temperature=0.5,\n",
    "        huggingfacehub_api_token=HF_TOKEN,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "\n",
    "hf_llm = load_llm(HUGGINGFACE_REPO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Database Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prompt for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "Use the pieces of information provided in the context to answer the user's question.\n",
    "If you don't know, just say so—don't fabricate.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer (concise, no chit-chat):\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=CUSTOM_PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RAG_as_Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_tool(vstore, llm):\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "    )\n",
    "\n",
    "    def _run(query: str) -> str:\n",
    "        out = rag_chain.invoke({\"query\": query})\n",
    "        answer = out[\"result\"]\n",
    "        sources = out.get(\"source_documents\", [])\n",
    "\n",
    "        source_texts = \"\\n\\n\".join(\n",
    "            f\"Source {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(sources)\n",
    "        )\n",
    "\n",
    "        return f\"{answer}\\n\\n---\\nSources:\\n{source_texts}\"\n",
    "\n",
    "    return Tool(\n",
    "        name=\"KB-RAG\",\n",
    "        description=\"Answer questions using the internal knowledge base (PDFs).\",\n",
    "        func=_run,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prompt_for_ReActive_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_PROMPT = \"\"\"\n",
    "You are a helpful assistant with access to these tools:\n",
    "{tool_names}\n",
    "Always greet professionally representing as an assistive Chatbot of ARB whenever user greets you.\n",
    "When you receive a question, first think about whether you need to use a tool.\n",
    "When you decide to call a tool mention the tool, use **exactly** this format (no variations):\n",
    "\n",
    "Thought: do I need to use a tool? yes\n",
    "Action: tool_name[tool input]\n",
    "\n",
    "When the tool returns, continue like this:\n",
    "\n",
    "Observation: tool output\n",
    "\n",
    "Repeat Thought/Action/Observation as needed.  \n",
    "When you are ready to answer the user, write:\n",
    "\n",
    "Thought: Got the final answer <using the tool name here>.\n",
    "Final answer: <your answer here, followed by “ Source: ” and the sources list including referred links, documents or any other resource>.\n",
    "\n",
    "Begin!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Content Safety as well. Look for AIGA Validation Framework\n",
    "#Login History of Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = TavilySearchResults(max_results=2)\n",
    "groq_llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "def get_response_from_ai_agent(\n",
    "    query: str,\n",
    "    *,\n",
    "    allow_search=True,\n",
    "    use_vector_db=True,\n",
    "    recursion_limit=40,\n",
    "):\n",
    "    tools = []\n",
    "    if allow_search:\n",
    "        tools.append(search_tool)\n",
    "    if use_vector_db:\n",
    "        tools.append(rag_tool)\n",
    "    #agent uses the Groq LLM\n",
    "    agent = create_react_agent(\n",
    "        model=groq_llm,\n",
    "        tools=tools,\n",
    "        messages_modifier=REACT_PROMPT\n",
    "    )\n",
    "\n",
    "    state = {\"messages\": [HumanMessage(content=query)]}\n",
    "    response = agent.invoke(state, config={\"recursion_limit\": recursion_limit})\n",
    "\n",
    "    ai_msgs = [m for m in response[\"messages\"] if isinstance(m, AIMessage)]\n",
    "    if not ai_msgs:\n",
    "        raise RuntimeError(\n",
    "            \"Agent didn’t finish. Dump:\\n\" + \"\\n\".join(str(m) for m in response[\"messages\"])\n",
    "        )\n",
    "\n",
    "    return ai_msgs[-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/7sp4jvq13bnfz2s5v60yf4p40000gn/T/ipykernel_50833/20080166.py:17: LangGraphDeprecationWarning: Parameter 'messages_modifier' in function 'create_react_agent' is deprecated as of version 0.1.9 and will be removed in version 0.3.0. Use 'state_modifier' parameter instead.\n",
      "  agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *ARB_Chatbot's Response:\n",
      " Hello! I'm an assistive Chatbot of ARB. It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "answer = get_response_from_ai_agent('''Hi!''')\n",
    "\n",
    "print(\"\\n *ARB_Chatbot's Response:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v5/7sp4jvq13bnfz2s5v60yf4p40000gn/T/ipykernel_50833/20080166.py:17: LangGraphDeprecationWarning: Parameter 'messages_modifier' in function 'create_react_agent' is deprecated as of version 0.1.9 and will be removed in version 0.3.0. Use 'state_modifier' parameter instead.\n",
      "  agent = create_react_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " *ARB_Chatbot's Response:\n",
      " Thought: Got the final answer using KB-RAG.\n",
      "Final answer: AD Reviews are typically conducted at various stages of the Life Cycle, including during the Initiation phase, Planning phase, Execution phase, and Closure phase. Source: KB-RAG\n"
     ]
    }
   ],
   "source": [
    "answer = get_response_from_ai_agent('''Sure, please tell me when AD Reviews are done in the Life Cycle''')\n",
    "\n",
    "print(\"\\n *ARB_Chatbot's Response:\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
